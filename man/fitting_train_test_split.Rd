% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fitting_train_test_split.R
\name{fitting_train_test_split}
\alias{fitting_train_test_split}
\title{Fitting a model on the training set and predicting the test set.}
\usage{
fitting_train_test_split(
  split,
  prediction_method = c("xgboost"),
  seed,
  inner_cv_reps = 2,
  inner_cv_folds = 5,
  ...
)
}
\arguments{
\item{prediction_method}{\code{character} Prediction method other than
kernel-based methods to use on the predictors defined by the recipe item of
the split object.Options are 'xgboost' for XGBoost, 'rf' for Random Forest.
Default is \code{xgboost}.}

\item{seed}{\code{integer} Seed value.}

\item{inner_cv_reps}{\code{integer} Number of times to repeat the k-fold
partitioning used for the inner cross-validation for estimation of the best
hyperparameters. Default is 2.}

\item{inner_cv_folds}{\code{integer} Number of partitions of the training set
for the inner cross-validation for estimation of the best hyperparameters.
Default is 5.}

\item{split.}{A \code{split_processed} object containing:
\describe{
\item{training}{\code{data.frame} Training set}
\item{test}{\code{data.frame} Test set}
\item{rec}{\code{recipe} object with the different steps to implement
as processing steps on the training dataset. Same transformations applied
on the test set.}
}}
}
\value{
a \code{list} object containing:
\describe{
\item{training}{\code{data.frame} Training set.}
\item{test}{\code{data.frame} Test set.}
\item{predictions_df}{\code{data.frame} with original test dataset with
extra column containing predicted values.}
\item{cor_pred_obs}{\code{numeric} Pearson's correlation between predicted
and observed values of the test set.}
\item{rmse_pred_obs}{\code{numeric} root mean square error between predicted and
observed values of the test set.}
\item{best_hyperparameters}{\code{tbl_df} the tuning parameter combination with
the best performance values which was used to fit the final model on the
training set.}
}
}
\description{
Function creating a workflow based on the prediction model chosen (all
methods other than multi-kernel learning), with hyperparameter optimization
using a Bayesian approach on the training set. Once the best hyperparameters
are identified via resampling, the model is fitted on the complete dataset.
Finally the test set is predicted.
}
\author{
Cathy C. Jubin \email{cathy.jubin@uni-goettingen.de}
}
